---
title: Operating Systems Notes
excerpt: couldn't get any worse than this right?
---
[DownloadMoreRAM.com - CloudRAM 2.0](https://downloadmoreram.com/)

Kelila‚Äôs Notes

[](https://www.notion.so/105fbf96867a8078b48fd3d8b2bf8863?pvs=21)

Norberth‚Äôs Notes

[https://docs.google.com/document/d/1wFayVkOesPq96KYeaOVXPEqpB8rB_HokiUk9sDUFIpw/edit?tab=t.57f9a3rnvlv9](https://docs.google.com/document/d/1wFayVkOesPq96KYeaOVXPEqpB8rB_HokiUk9sDUFIpw/edit?tab=t.57f9a3rnvlv9)

Regular Students Notes

[Operation System üíª](https://www.notion.so/Operation-System-12b1620f860e80cdb47aff5eb21febb8?pvs=21)

[](https://www.notion.so/f97c874f97bc48ab9944243d4380d3bd?pvs=21)

[OS UTS Notes](https://www.notion.so/OS-UTS-Notes-12f40cf9fbe080268fcfe177728b6e94?pvs=21)

- 2023 UTS Responsi key points i guess
    
    > embdedded OS
    > 
    > concurrency
    > 
    > basic elements
    > 
    > architecture operating systems windows linux android
    > 
    > multiprocessor
    > 
    > thread
    > 
    > process description and control
    > 
    > fork()
    > 
    > process scheduling gant chart
    

`\\\\TODO`

- [x] WATCH UI RECORDINGS
    - [x] Week 8
- [x] WATCH RESPONSI [https://www.youtube.com/watch?v=N0u8MfiMWd0&ab_channel=HIMTIBINUS](https://www.youtube.com/watch?v=N0u8MfiMWd0&ab_channel=HIMTIBINUS)
- [x] Complete Notes
- [x] Try GPT Exercise
- [x] Try Norberth Theory GPT Exercise
- [x] MICKO BRING MY CALCULATOR

## Resources

<aside> üìÑ

[INTRO.pdf](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/8482c7d2-c700-474f-be6b-6199ce55c7ab/INTRO.pdf)

</aside>

<aside> üìÑ

[PROCESS.pdf](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/48782ac7-0a55-4013-a8fa-f4b21dc7dbe9/PROCESS.pdf)

</aside>

<aside> üìÑ

[THREADS CONCURRENCY SYNC.pdf](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/0982b35e-be6a-468e-8fef-c677eb487ea2/THREADS_CONCURRENCY_SYNC.pdf)

</aside>

<aside> üìÑ

[KLT ULT.pdf](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/9ecd9fb7-8b55-4a00-aaee-1e1212626b01/KLT_ULT.pdf)

</aside>

<aside> üìÑ

[INTERRUPTS.pdf](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/fb8e98f2-9a10-47e1-8a07-3491adb288c4/INTERRUPTS.pdf)

</aside>

<aside> üìÑ

[DEADLOCK.pdf](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/78edf163-48c7-4052-83cd-e9effe5b5600/DEADLOCK.pdf)

</aside>

## Interrupts (TO QUIZ)

<aside> üìù

### Type of Interruption

Interrupts can be broadly categorized based on their causes, each enabling the processor to handle different types of events more efficiently by temporarily suspending the current task to address specific needs. The main types include:

1. **Program Interrupts**: These occur due to errors during instruction execution, such as arithmetic overflow, division by zero, illegal instructions, or memory access violations. They allow the system to manage such exceptions by stopping the program and signaling for corrective actions.
2. **Timer Interrupts**: Generated by an internal timer, these interrupts enable the operating system to perform periodic tasks, like managing system processes or resource allocation, by triggering at regular intervals.
3. **I/O Interrupts**: Issued by I/O devices, these interrupts signal the completion of data transfers or indicate errors in device operations. By using I/O interrupts instead of waiting (polling), the processor can continue other tasks until the device signals it has completed, enhancing efficiency.
4. **Hardware Failure Interrupts**: Triggered by hardware issues like power failures or memory parity errors, these interrupts are critical for protecting system data and ensuring a safe shutdown or alert for immediate corrective actions.

Overall, each type of interrupt helps optimize processor utilization by addressing specific events without disrupting the primary user program's flow more than necessary.

</aside>

<aside> üìù

### Registers and Interrupts. **How do interrupts work?**

When an interrupt occurs, the processor completes its current instruction before acknowledging the interrupt signal from an I/O device. It checks for the pending interrupt, sends an acknowledgment to the device, and saves essential state information to resume the interrupted program later. Specifically, the Program Status Word (PSW), which contains details like condition codes and memory usage, and the Program Counter (PC), holding the address of the next instruction, are both pushed onto a control stack. The processor then loads the PC with the address of the interrupt handler, transferring control to it. Once in the handler, all processor registers are saved to prevent data loss, allowing the handler to examine the interrupt cause, possibly sending additional commands or acknowledgments to the I/O device. After processing, the handler retrieves all saved registers and restores the PSW and PC from the stack, allowing the interrupted program to resume precisely where it left off, unaffected by the interrupt.

- Detailed Explanation
    
    > When an interrupt occurs, it initiates a sequence of events in both the hardware and software of the processor to handle the interrupt efficiently. Here‚Äôs a technical breakdown of what happens, linked to the specific registers involved:
    > 
    > 1. **Interrupt Signal and Execution Completion**: When an I/O device issues an interrupt, the processor completes the current instruction before acknowledging the interrupt. This ensures no partial instruction execution when switching tasks.
    >     
    > 2. **Interrupt Detection and Acknowledgment**: The processor checks for a pending interrupt, then sends an acknowledgment signal to the device. This acknowledgment lets the device stop sending the interrupt signal, confirming that the processor is ready to handle it.
    >     
    > 3. **Saving State Information (PSW and PC)**: To handle the interrupt without losing progress on the current program, the processor saves critical information. This includes:
    >     
    >     - **Program Status Word (PSW)**: The PSW holds the current state of the process, such as condition codes, memory usage, and whether the interrupt system is enabled.
    >     - **Program Counter (PC)**: The PC stores the address of the next instruction in the current program.
    >     
    >     Both the PSW and PC are pushed onto a control stack, which temporarily holds this state information so the processor can return to the interrupted program afterward.
    >     
    > 4. **Loading the Interrupt Handler**: The processor loads the PC with the address of the appropriate interrupt handler. This depends on the system design‚Äîeither one handler for all interrupts, separate handlers for each interrupt type, or even specific handlers per device. The loaded PC redirects the next instruction fetch to the interrupt handler, effectively transferring control to it.
    >     
    > 5. **Saving Additional Register Values**: Once control has passed to the interrupt handler, all active processor registers are saved to the stack. This preserves the complete program state, allowing the interrupt handler to freely use these registers without affecting the interrupted program‚Äôs operation.
    >     
    > 6. **Executing the Interrupt Handler**: The handler examines the cause of the interrupt, checks the status information for the device that triggered it, and may communicate further with the device (e.g., sending commands or responses).
    >     
    > 7. **Restoring Registers and Resuming the Program**: After handling the interrupt, the saved register values, along with the PSW and PC, are restored from the stack. This restoration is crucial as it reinstates the exact state of the interrupted program, allowing it to continue from the point of interruption without any disruption.
    >     
    > 
    > In summary, the PSW and PC are essential registers in interrupt processing, as they allow the processor to save and later restore the exact state of the program that was running when the interrupt occurred.
    

</aside>

## Process States (TO PRACTICE)

<aside> üìù

Application is a program on disk, flash memory which is not executing (static entity)

Process is a state of a program when executing loaded in memory (active entity)

Can have multiple processes of same program but with different states

</aside>

![Screenshot 2024-11-04 at 10.44.23 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/2bb9e220-6b7a-4d6f-ad7f-9352dfd8a182/Screenshot_2024-11-04_at_10.44.23_PM.png)

<aside> üìù

> ### Summary of Suspended Processes and the Need for Swapping
> 
> In operating systems, processes typically operate in three states: **Ready**, **Running**, and **Blocked**. However, to effectively manage memory, especially in systems without virtual memory, it becomes necessary to introduce additional states‚Äîspecifically, the **Suspend** states.
> 
> **Understanding Swapping**:
> 
> 1. **Context of Swapping**: When a system lacks virtual memory, it requires that all processes must be loaded into main memory to execute. In such cases, if all processes are blocked (waiting for I/O operations), the CPU can become idle because there's no ready process to execute.
> 2. **Drawbacks of Larger Main Memory**: While expanding main memory to accommodate more processes could seem like a solution, it has two significant downsides:
>     - **Cost**: The expense of memory increases with size, even if the cost per byte is low.
>     - **Memory Demand**: As software grows, so do their memory requirements, meaning larger memory often leads to larger processes, not more processes.
> 3. **Swapping Mechanism**: To address these issues, **swapping** is introduced. This involves moving a process that is blocked out of main memory and onto disk storage (in a suspend queue) when there are no ready processes available. This allows the OS to load another process from the suspend queue or accommodate new process requests.
> 4. **Adding the Suspend State**: With swapping, a new state is added to the process model:
>     - **Suspend**: This state indicates a process that has been temporarily moved to disk, freeing up memory for other processes.
> 
> **State Descriptions**:
> 
> To facilitate this model, we define four key states:
> 
> 1. **Ready**: The process is in main memory and ready for execution.
> 2. **Blocked**: The process is in main memory but waiting for an event (like I/O completion).
> 3. **Blocked/Suspend**: The process is on disk and waiting for an event.
> 4. **Ready/Suspend**: The process is on disk but can be executed once it is loaded into memory.
> 
> **Process State Transitions**:
> 
> - When no processes are in the Ready state, a blocked process may be swapped out to the disk to free up memory.
> - Once the event that a suspended blocked process was waiting for occurs, it can transition back to the Ready state.
> - Processes can transition between states based on their activity and system needs. For instance, a new process can go to either the Ready or Ready/Suspend state, depending on memory availability.
> 
> **Generalization of Suspended Processes**:
> 
> Suspended processes are defined by their unavailability for immediate execution, regardless of whether they are waiting on an event. This state can be induced by the OS or parent processes and must be explicitly resolved to return to execution. Reasons for suspension can include:
> 
> - Freeing memory for other processes.
> - Monitoring system resource utilization or performance tracking.
> 
> ### Conclusion
> 
> In essence, managing process states, particularly through the introduction of suspend states and swapping mechanisms, is crucial for optimizing CPU usage and memory management in operating systems. By allowing processes to be temporarily stored on disk when not immediately needed, systems can maintain efficient execution without requiring constant access to main memory.

</aside>

<aside> üìù

Here‚Äôs a similar question along with a detailed solution to help explain how to analyze process states based on a given timeline of events:

---

**Example Question:**

Several processes are submitted to a computer system. Assume that the processor is initially idle, and the following processes are submitted:

i. At time=0, process X is submitted. Since the processor is available, the dispatcher selects process X to execute.

ii. At time=2, process Y is submitted.

iii. At time=3, process Z is submitted.

iv. At time=4, process X requests a "read".

v. At time=6, the computer runs out of memory, and the swapper swaps the process image of X into the secondary memory.

vi. At time=7, process Y requests a "write".

vii. At time=9, the read request of X is provided.

viii. At time=10, process W arrives.

Based on the above processes, write down the process states of **each process** at time=5 and time=11.

---

### Solution:

1. **Analyze the Timeline Events**:
    - **At time=0:** Process X is submitted and starts executing (Running).
    - **At time=2:** Process Y is submitted (Ready).
    - **At time=3:** Process Z is submitted (Ready).
    - **At time=4:** Process X requests a "read" (Blocked, waiting for I/O).
    - **At time=6:** Process X is swapped out to secondary memory due to memory shortage (Blocked/Suspended).
    - **At time=7:** Process Y requests a "write" (Blocked, waiting for I/O).
    - **At time=9:** The read request of Process X is provided (Process X can be unblocked, but it is suspended in memory).
    - **At time=10:** Process W arrives (Ready).

### Process States at Time = 5

- **Process X:** **Blocked/Suspended** (Since Process X requested a "read" at time 4 and is swapped out to secondary memory at time 6, it remains in a blocked state).
- **Process Y:** **Ready** (It has arrived and is ready to be scheduled but is currently not executing).
- **Process Z:** **Ready** (It has arrived and is also ready to be scheduled).
- **Process W:** Not yet arrived, so **Not Submitted**.

### Process States at Time = 11

- **Process X:** **Ready/Suspended** (At time 9, the read request for X is provided, so it can transition to the ready state, but it remains suspended in secondary memory).
- **Process Y:** **Blocked** (waiting for its "write" operation to complete).
- **Process Z:** **Ready** (still in the Ready state as it has not been scheduled or blocked).
- **Process W:** **Ready** (it has just arrived and is available for execution).

### Summary of Process States

- **At time = 5:**
    - Process X: Blocked/Suspended
    - Process Y: Ready
    - Process Z: Ready
    - Process W: Not Submitted
- **At time = 11:**
    - Process X: Ready/Suspended
    - Process Y: Blocked
    - Process Z: Ready
    - Process W: Ready

---

This structured approach allows you to track the states of processes based on the given timeline and transitions between states due to events such as I/O requests and memory management.

</aside>

## Deeply Embedded Systems (TO QUIZ)

<aside> ‚ùó

refer to GSLC

- farrel
    
    Embedded system refers to the use of computer system within a larger mechanical or electrical system that has a specific function or set of functions. Meanwhile a deeply embedded system is a subset of embedded system which are more tightly integrated to the hardware they control. Some differences include:
    
    - Embedded systems generally have more resources such as memory and processing power, while deeply embedded systems have extreme resource constraints in terms of memory, processor size, time, and power consumption.
    - Deeply embedded systems are more dedicated and designed for very specific singular tasks with minimal functionality. On the other hand, embedded systems can handle more complex tasks and may tun a full operating system
    - Embedded systems often have some form of UI while deeply embedded systems typically have no direct user interface.
- mines
    
    An embedded system is a specialized computing system designed to perform dedicated functions within a larger mechanical or electrical system. These systems can range from simple devices with minimal processing power to complex setups capable of running full operating systems and handling multiple tasks. They often include user interfaces for user interaction and have more available resources like memory and processing power. In contrast, a deeply embedded system is a subset of embedded systems that is tightly integrated with the hardware it controls and is optimized for very specific, singular tasks. These systems operate under strict resource constraints in terms of memory, processor size, power consumption, and typically lack a direct user interface, functioning autonomously without user input. They are often inseparable from the main system and designed for real-time processing. Key differences include resource availability (embedded systems have more, deeply embedded systems have less), functionality (embedded systems handle more complex tasks, deeply embedded systems focus on minimal, specific functions), user interaction (embedded systems often have user interfaces, deeply embedded systems do not), and system integration (deeply embedded systems are more tightly integrated with hardware).
    

</aside>

<aside> üìù

A deeply embedded system is a specialized, single-purpose device with a microcontroller that operates autonomously, often without any user interaction or reprogramming. These systems are embedded within larger devices, executing tasks based on specific inputs from their environment, such as sensor readings, and are optimized for minimal resource usage in terms of memory, processing power, and energy. They are not general-purpose computers; instead, they run pre-loaded programs stored in ROM, making them non-programmable once deployed. A common example is a network of sensors in agriculture that autonomously collect and transmit environmental data, operating independently with strict resource constraints, which is essential for Internet of Things (IoT) and automated applications.

- Detailed Explanation
    
    > A **deeply embedded system** is an embedded system that operates independently and has minimal or no interaction with users. Here‚Äôs a breakdown of the concept and an example:
    > 
    > ### What is a Deeply Embedded System?
    > 
    > - **Definition**: Deeply embedded systems are dedicated, single-purpose devices with embedded microcontrollers (rather than microprocessors) that perform specific tasks autonomously.
    > - **Characteristics**:
    >     - **Limited Observability**: The behavior of a deeply embedded system is not easily observable by users or programmers.
    >     - **No Programmability**: Once the program is loaded into the device‚Äôs ROM (read-only memory), it cannot be altered.
    >     - **Minimal or No User Interaction**: Many deeply embedded systems don‚Äôt have interfaces for users. They simply perform their tasks based on sensor inputs or other external triggers.
    >     - **Resource-Constrained**: These systems typically have strict limits on memory, processing power, and energy consumption.
    > 
    > ### Example of a Deeply Embedded System
    > 
    > - **Example**: Sensor networks used in agriculture or industrial automation.
    >     - **How it Works**: In an agricultural field, a network of deeply embedded sensors monitors soil moisture, temperature, and humidity. Each sensor has a microcontroller programmed to collect data, process it minimally, and send it to a central base station.
    >     - **Why It‚Äôs Deeply Embedded**: These sensors operate independently, with no human interaction or reprogramming. They rely on minimal resources, only consuming power as needed, and often include wireless capabilities to relay data over a network without external intervention.
    > 
    > ### Summary
    > 
    > Deeply embedded systems perform specific, autonomous tasks with minimal resource requirements. They are widely used in environments like sensor networks, where human interaction and reprogramming are not required, making them vital in the Internet of Things (IoT) and automation applications.
    
- Book Definition
    
    > A large percentage of the total number of embedded systems are referred to as deeply embedded systems. Although this term is widely used in the technical and commercial literature, you will search the Internet in vain (at least the writer did) for a straightforward definition. Generally, we can say a deeply embedded system has a processor whose behavior is difficult to observe both by the programmer and the user. A deeply embedded system uses a microcontroller rather than a microprocessor, is not programmable once the program logic for the device has been burned into ROM (read-only memory), and has no interaction with a user. Deeply embedded systems are dedicated, single-purpose devices that detect something in the environment, perform a basic level of processing, then do something with the results. Deeply embedded systems often have wireless capability and appear in networked configurations, such as networks of sensors deployed over a large area (e.g., factory, agricultural field). The Internet of Things depends heavily on deeply embedded systems. Typically, deeply embedded systems have extreme resource constraints in terms of memory, processor size, time, and power consumption.
    
- Chapter Summary
    
    > Here‚Äôs a concise summary of the key points about **embedded systems**, **TinyOS**, and **embedded Linux**:
    > 
    > ### Embedded Systems
    > 
    > - **Definition**: An embedded system is a specialized computer within a device performing specific tasks, unlike general-purpose computers like laptops.
    > - **Components**: They often include processors, memory, sensors, and actuators to interact with their environment.
    > - **Characteristics**: Real-time operations, efficient use of energy, minimal human interaction, and high configurability. Examples include devices like phones, cars, home appliances, and medical devices.
    > 
    > **Microcontrollers vs. Microprocessors**:
    > 
    > - **Microcontrollers**: Include processor, memory, and I/O on a single chip; typically used for specific tasks.
    > - **Microprocessors**: Often require external components (e.g., memory) and can support complex, multi-function applications.
    > 
    > ### Embedded Operating Systems
    > 
    > - **Characteristics**: These systems often support real-time operation, flexible I/O, minimal protection mechanisms, and direct interrupt handling. They can be **custom-built OS** (e.g., TinyOS) or **adapted OS** (e.g., embedded Linux).
    > - **Host-Target Development**: Software is developed on one platform (host) and runs on another (target), using tools like cross-compilers.
    > 
    > ### Embedded Linux
    > 
    > - **Definition**: A variant of Linux optimized for embedded systems, often pared down to meet specific hardware constraints.
    > - **Characteristics**: Highly configurable kernel, support for real-time requirements, network capabilities, minimal file systems (e.g., `cramfs` and `squashfs`).
    > - **Advantages**: Cost-effective, vendor-independent, supports diverse hardware, and benefits from open-source contributions.
    > - **mClinux**: A smaller, embedded Linux version tailored for microcontrollers, lacking memory management features like dynamic address spaces, making it efficient for small devices.
    > 
    > ### TinyOS
    > 
    > - **Purpose**: Designed specifically for wireless sensor networks and small embedded systems with tight power and resource constraints.
    > - **Structure**: Component-based OS, using modules for tasks like networking, power management, and timing, without a traditional kernel.
    > - **Concurrency**: Uses a non-blocking, event-driven model with tasks running to completion.
    > - **Scheduler**: A simple FIFO scheduler that puts the system to sleep when idle, conserving power.
    > - **Example Use Case**: In sensor networks, TinyOS components manage tasks like data collection, signal processing, and communication with minimal latency and power usage.
    > 
    > TinyOS is especially effective in **wireless sensor networks** due to its highly modular structure, low power requirements, and ability to handle high concurrency, making it ideal for monitoring and ad-hoc data transmission in low-power environments.
    

</aside>

## Process Control Block

<aside> üìù

![Screenshot 2024-10-20 at 9.40.04 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/7a509af8-1faa-4920-ad72-653cf84d679b/Screenshot_2024-10-20_at_9.40.04_PM.png)

</aside>

## Forking (TO PRACTICE)

[Online C++ Compiler - Programiz](https://www.programiz.com/cpp-programming/online-compiler/)

[GDB online Debugger | Compiler - Code, Compile, Run, Debug online C, C++](https://www.onlinegdb.com/)

<aside> üìù

![fork returns child pid to parent and 0 to child](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/7c33c5c4-ca6e-435f-ba2e-dcbf3b42d361/Screenshot_2024-11-03_at_12.19.32_PM.png)

fork returns child pid to parent and 0 to child

[Recording Week07: Process & Cnncurrency](https://youtu.be/-pL2fAdb7Kw?si=0CHYL3PosxfeCFRg&t=1378)

- **Extra Forking Practice 1**
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/5a8ff96d-4a03-4104-b8e1-459b394ce3ce/image.png)
    
    Creating processes using the fork() system call in a certain way, will result in a hierarchy of processes.
    
    Write a C program to create such a hierarchy. Each child simply display its process id.
    
    - Answer
        
        ![92bb3113-69e7-4655-94b7-2fc26b57989c.sketchpad (3).jpeg](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/c77238e2-2ad5-404e-abbb-d77716a91afd/92bb3113-69e7-4655-94b7-2fc26b57989c.sketchpad_(3).jpeg)
        
        ```c
        #include <stdio.h>
        #include <unistd.h>
        #include <sys/wait.h>
        void show(int i){
            printf("%d, %d %d\\n", 
            getpid(), getppid(), i);
        }
        int main(){
            pid_t pid_2, pid_3, pid_4;
            pid_2 = fork();
            if(pid_2 == 0){
                pid_3 = fork();
                if(pid_3 == 0) show(3);
                // print p3
                else {
                    pid_4 = fork();
                    if(pid_4 == 0) show(4);
                    // print p4
                    else {
                        show(2);
                        // print p2
                        wait(NULL);
                    }
                }
            }
            else{
                show(1);
                // print p1
                wait(NULL);
            }
            return 0;
        }
        ```
        

</aside>

## Kernel Level and User Level Threads (TO QUIZ ‚è™)

<aside> üìù

> Kernel-level and user-level threads represent two primary approaches to threading in operating systems, each with unique advantages and limitations. **User-level threads (ULTs)** are managed entirely in the application layer using a threading library, which handles scheduling, synchronization, and switching between threads without involving the OS kernel. This allows ULTs to avoid costly system calls, making thread management faster. They are also OS-independent and can implement application-specific scheduling. However, ULTs face challenges: if one thread blocks due to a system call, all threads within the process block, and ULTs cannot fully utilize multiple CPUs since the OS treats the process as a single execution unit. **Kernel-level threads (KLTs)**, on the other hand, are managed by the OS kernel. The kernel is aware of each thread, allowing it to schedule multiple threads from the same process across different CPUs and manage blocked threads independently. This enables KLTs to take advantage of multiprocessor systems effectively. However, each thread operation (like switching) involves a mode switch to the kernel, which incurs performance overhead. **Combined Approaches** (such as Solaris) attempt to leverage the best of both methods by mapping multiple user-level threads onto kernel threads, balancing efficient user-level control with kernel-level support to handle system calls and multiprocessing.

- OS must have special priveleges to have direct access to hardward
    - thus operates in kernel space
- Applications operates in user space unprivelege mode
- in kernel mode any instruction that manipulates hardware is permitted to execute
    - in user level it is forbidden and will cause a trap, application will be interrupted and hardware will switch the control to the OS and check whether to grant access or terminate process
- interaction between app and OS can be via system call
- arguments can be passed directly from program to OS or indirectly by specifying their address using registers
- it will effect the cache when transitioning (being replaced)

User Level Threads

mulitple ULT on one KLT

- All thread management is done by the application
    
- The kernel is not aware of the existence of threads
    
    Advantages
    
    - Scheduling can be application specific
    - ULTs can run on any OS
    - Thread switching does not require kernel mode privileges
    
    Disadvantages
    
    - In a typical OS many system calls are blocking "As a result, when a ULT executes a system call, not only is that thread blocked, but all of the threads within the process are blocked as well
    - In a pure ULT strategy, a multithreaded application cannot take advantage of multiprocessing "A kernel assigns one process to only one processor at a time, therefore, only a single thread within a process can execute at a time
    - Jacketing Purpose is to convert a blocking system call into a non-blocking system call Writing an application as multiple processes rather than multiple threads However, this approach eliminates the main advantage of threads

Kernel Level Thread

One ULT for One KLT

- Thread management is done by the kernel
    
- There is no thread management code in the application level, simply an application programming interface (API) to the kernel thread facility "Windows is an example of this approach
    
    Advantages
    
    - The kernel can simultaneously schedule multiple threads from the same process on multiple processors
    - If one thread in a process is blocked, the kernel can schedule another thread of the same process
    - Kernel routines themselves can be multithreaded
    
    Disadvantages
    
    - All must follow one to one
    - Scheduling should be the same
    - The transfer of control from one thread to another within the same process requires a mode switch to the kernel

Combined Approaches

- Thread creation is done completely in the user space, as is the bulk of the scheduling and synchronization of threads within an application

### 5.31-5.33

Kernel Level Threads - OS itself is multithreaded, visible to the kernel, managed by kernel level components ex: kernel level scheduler

User Level Threads - Processes are multithreaded, must be associated with a kernel level thread then OS level scheduler must schedule kernel level thread onto a CPU

> Key Concepts:
> 
> 1. **Kernel-level threads**: Managed by the OS itself, visible to the OS scheduler.
> 2. **User-level threads**: Managed by the application, often invisible to the OS, relying on a thread library in user space.
> 
> ### Threading Models:
> 
> ### 1. **One-to-One Model**
> 
> - **How it Works**: Each user-level thread has a corresponding kernel-level thread. When a user thread is created, it is paired with a kernel thread that the OS can manage.
> - **Why It‚Äôs Useful**: The OS has full visibility into each user-level thread, allowing it to handle synchronization, blocking, and scheduling effectively.
> - **Drawbacks**: Every operation requires a system call (moving from user to kernel space), which can be slow. It‚Äôs also limited by the OS's thread management policies, meaning if the OS limits the number of threads, so does the application.
> 
> ### 2. **Many-to-One Model**
> 
> - **How it Works**: Multiple user-level threads are mapped to a single kernel-level thread. The user-level threading library manages scheduling and synchronization, deciding which user thread runs.
> - **Why It‚Äôs Useful**: This model is highly portable, as it doesn‚Äôt depend on the OS for threading support. There‚Äôs no need for system calls when managing threads, so it‚Äôs faster in some contexts.
> - **Drawbacks**: Since the OS only sees one kernel thread, if this thread is blocked (e.g., waiting for I/O), _all_ user threads are also blocked. The OS can‚Äôt tell that there are multiple threads, so it can‚Äôt manage them individually.
> 
> ### 3. **Many-to-Many Model**
> 
> - **How it Works**: Multiple user threads are mapped to multiple kernel threads. The OS is aware of multiple threads, and some user threads may be permanently paired (or ‚Äúbound‚Äù) with kernel threads for faster handling.
> - **Why It‚Äôs Useful**: This model combines flexibility and efficiency. If one user thread is blocked, others can continue to run because there are multiple kernel threads available.
> - **Drawbacks**: Requires coordination between the OS and the user-level thread manager, which can add complexity. However, this coordination allows the OS to optimize performance.
> 
> ### Summary:
> 
> - **One-to-One**: OS manages each user thread with a corresponding kernel thread; good control but can be slow due to system calls.
> - **Many-to-One**: Fast, doesn‚Äôt depend on OS thread support, but all threads are blocked if one is blocked.
> - **Many-to-Many**: Balances flexibility and control, allowing some threads to keep running if others are blocked, but requires more coordination between the OS and the user-level thread manager.

> Let‚Äôs expand on the explanation by incorporating this background on kernel-level and user-level thread support, providing a fuller picture of how threads work in each model:
> 
> ### Thread Support at Different Levels
> 
> Threads can be managed at either the **kernel level** or the **user level**, or both. Here‚Äôs what each involves:
> 
> 1. **Kernel-Level Threads**:
>     - Managed directly by the OS kernel, which means the OS itself is multithreaded.
>     - The OS kernel keeps a data structure to represent each thread and handles operations like synchronization and scheduling.
>     - This structure allows the OS to manage how threads share CPU and memory resources.
> 2. **User-Level Threads**:
>     - Managed by a user-level thread library linked to the application.
>     - The library provides support for thread creation, scheduling, and synchronization without involving the OS.
>     - Different applications can use different user-level libraries, allowing unique ways of handling threading (e.g., custom scheduling methods).
> 
> These approaches combine in the following threading models, which outline how user-level threads can map onto kernel-level threads.
> 
> ### Threading Models
> 
> 1. **One-to-One Model**:
>     - Each user thread has a matching kernel thread. The OS is fully aware of every user thread, meaning it can control their scheduling, synchronization, and blocking.
>     - **Pros**: Since the OS can ‚Äúsee‚Äù all threads, it manages each directly, offering robust control.
>     - **Cons**: Each thread creation or operation requires a system call, which can slow things down. The application also depends on the OS for thread limits and policies, which may limit portability.
> 2. **Many-to-One Model**:
>     - Multiple user-level threads are mapped to a single kernel-level thread. A user-level library handles scheduling and synchronization, invisible to the OS.
>     - **Pros**: Efficient, since no system calls are needed for thread operations; supports portability across OSes.
>     - **Cons**: The OS sees only one kernel thread, so if it gets blocked (e.g., by an I/O request), all user threads are also blocked. The OS is unaware of the internal thread activity.
> 3. **Many-to-Many Model**:
>     - Multiple user threads are mapped to multiple kernel threads, allowing some flexibility. Some user threads may be bound (one-to-one) to specific kernel threads, while others may share them.
>     - **Pros**: Offers a balance of flexibility and control. If one user thread is blocked, others can still execute since multiple kernel threads are available.
>     - **Cons**: Requires coordination between the OS and the user-level thread manager, adding complexity. But this complexity allows the OS to optimize resource management.
> 
> ### Summary:
> 
> - **Kernel-Level Threads**: Managed by the OS, giving it control over resources but introducing system call overhead.
> - **User-Level Threads**: Managed by the application‚Äôs library, providing flexibility but limiting OS visibility.
> 
> These **threading models** (One-to-One, Many-to-One, Many-to-Many) allow different balances between efficiency and control. By choosing a model, applications can optimize for factors like performance, portability, or control over the threading behavior.

> Here‚Äôs an expanded explanation that adds details about how single-threaded and multi-threaded processes work, and how these components are managed within the OS:
> 
> ### Single-Threaded vs. Multi-Threaded Processes
> 
> In a **single-threaded process**, the process includes:
> 
> - **Address space**: This maps virtual to physical memory addresses.
> - **Stack and registers**: Used for holding function calls and variables.
> - **Process Control Block (PCB)**: Contains all information about the process's state, including memory mappings, execution state, and resources.
> 
> When this process makes a system call, it enters the kernel, which uses a **kernel-level thread** to execute the request.
> 
> ### Making a Process Multi-Threaded
> 
> In a multi-threaded process, the structure becomes more complex. A **user-level threading library** manages multiple user-level threads within the process, but the OS only sees a single kernel thread for the process in some models (e.g., many-to-one).
> 
> ### Many-to-One Model:
> 
> - The user-level library represents threads using data structures to track scheduling, resource usage, and synchronization.
> - Multiple user threads are managed by a single kernel thread. The OS doesn‚Äôt ‚Äúsee‚Äù each user thread individually, only the kernel thread that represents the process.
> 
> ### One-to-One and Many-to-Many Models:
> 
> - For models with multiple kernel threads, the **PCB** (Process Control Block) structure is adjusted:
>     - **PCB** still holds information relevant to the entire process, such as virtual memory mappings.
>     - Each **kernel thread** also maintains its own stack and register data for its execution state, meaning we don‚Äôt need a separate PCB for each user thread.
> 
> ### User-Level Threads and Virtual CPUs
> 
> From the perspective of the user-level threading library, kernel threads appear like "virtual CPUs." This library schedules user threads onto these kernel threads based on available resources, managing context-saving and restoring operations (e.g., using Unix operations like `setjump` and `longjump` for switching contexts within the same address space).
> 
> ### Managing Multiple Processes
> 
> When dealing with multiple processes:
> 
> - Each process has its own PCB, user-level thread data structures, and, if applicable, multiple kernel threads.
> - Relationships are established to:
>     - **Track user threads** within each process.
>     - **Link kernel threads** to their respective PCBs, showing the address space and process each thread is part of.
> 
> ### Handling Multiple CPUs
> 
> If the system has multiple CPUs:
> 
> - The OS uses data structures to track each CPU and its associated kernel threads.
> - Each CPU has information on its current or recently scheduled kernel threads, maintaining **affinity** for better performance (where threads prefer running on specific CPUs).
> 
> ### Context Switching Between Kernel Threads
> 
> When the kernel switches between threads belonging to different processes:
> 
> - The OS recognizes they belong to different PCBs.
> - It saves the state of the first process‚Äôs PCB, invalidates current memory mappings, and restores those of the new process, ensuring each thread has the correct context.
> 
> ### Summary
> 
> - **Single-threaded processes** have one PCB and kernel thread.
> - **Multi-threaded processes** involve multiple user threads managed by either a single or multiple kernel threads, depending on the model.
> - In **multi-CPU systems**, kernel threads are assigned to specific CPUs, maintaining performance by preserving affinity.
> 
> Each model (one-to-one, many-to-one, many-to-many) balances visibility, flexibility, and control over threading to suit different needs in multitasking and resource management.

</aside>

## First Come First Serve (TO PRACTICE)

<aside> üìù

- non-preemptive
- whichever comes first will be executed first
- arrives in batch so there is no arrival time
- will execute according to the order of the process

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/08d0c823-62c5-4d73-bb7e-c4273b054e16/image.png)

```c
P1 ----
P2     ----
P3         ----
	 0  24  27  30
```

> Turnaround Time = Completion Time ‚Äì Arrival Time Waiting Time = Turnaround Time ‚Äì Burst Time

|Process|Burst Time|Turnaround Time|Waiting Time|
|---|---|---|---|
|P1|24|24 - 0 = 24|24 - 24 = 0|
|P2|3|27 - 0 = 27|27 - 3 = 24|
|P3|4|30 - 0 = 30|30 - 3 = 27|

$ATT = \frac{24+27+30}{3}=27$

$AWT = \frac{0 + 24 + 27}{3}=17$

- **Exercise**
    
    |Process|Burst Time|Turnaround Time|Waiting Time|
    |---|---|---|---|
    |P1|8|8 ‚Äì 0 = 8|8 ‚Äì 8 = 0|
    |P2|3|11 ‚Äì 0 = 11|11 - 3 = 8|
    |P3|6|17 - 0 = 17|17 ‚Äì 6 = 11|
    |P4|4|21 - 0 = 21|21 - 4 = 17|
    
    ```c
    P1 ----
    P2     ----
    P3         ----
    P4             ----
       0  8  11  17  21
    ```
    
    $ATT = \frac{8+11+17+21}{4}=14.25$
    
    $AWT = \frac{0+8+11+17}{4}=\frac{36}{4} = 9$
    
- Exercise UI
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/3d67344b-6238-4cb3-b6f8-85260c18a3a5/image.png)
    

</aside>

## Shorter Process Next/Shortest Job First (TO PRACTICE)

<aside> üìù

- non-preemptive
- short process will jump to the front of the queue

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/168c870a-0756-41e1-a6b8-bfb4841a0a5e/image.png)

```c
P1 ----
P2         ----
P3     ----
P4             ----
	 0  7   8   12  16   
```

> ‚Üí the 1st process will always start first because at it‚Äôs arrival time, it is the shortest process
> 
> ‚Üí skips longer processes and conducts shorter ones first
> 
> ‚Üí P2 before P4 even though the time is the same bcs P2 comes first

> Turnaround Time = Completion Time ‚Äì Arrival Time Waiting Time = Turnaround Time ‚Äì Burst Time

|Process|Burst Time|Arrival Time|Turnaround Time|Waiting Time|
|---|---|---|---|---|
|P1|7|0|7 - 0 = 7|7 - 7 = 0|
|P2|4|2|12 - 2 = 10|10 - 4 = 6|
|P3|1|4|8 - 4 = 4|4 - 1 = 3|
|P4|4|5|16 - 5 = 11|11 - 4 = 7|

$ATT = \frac{7+10+4+11}{4} = 8$

$AWT = \frac{0+6+3+7}{4}=4$

- **Exercise**
    
    |Process|Burst Time|Arrival Time|Turnaround Time|Waiting Time|
    |---|---|---|---|---|
    |P1|8|0|8 - 0 = 8|8 - 8 = 0|
    |P2|3|2|11 - 2 = 9|9 - 3 = 6|
    |P3|6|4|21 - 4 = 17|17 - 6 = 11|
    |P4|4|5|15 - 5 = 10|10 - 4 = 6|
    
    ```c
    P1 ----
    P2     ----
    P3             ----
    P4         ----    
       0   8  11  15  21
    ```
    
    $ATT = \frac{8+9+17+10}{4} = 11$
    
    $AWT = \frac{0+6+11+6}{4}=5.75$
    
- Exercise UI
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/265ee901-2536-4426-bcc7-5088aa60d2f4/image.png)
    

</aside>

## **Shortest Remaining Time (TO PRACTICE)**

<aside> üìù

- preemptive
- chooses process that has the shortest remaining processing time
- can stop a long process when a shorter one comes along

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/168c870a-0756-41e1-a6b8-bfb4841a0a5e/image.png)

```c
P1 ----                ----
P2     ----    ----
P3         ----    
P4                 ----
   0  2   4   5   7   11  16
2 4 5 7 11
5 5 5 5 5
4 2 2 0 0
  1 0 0 0 
    4 4 0
```

> ‚Üí P1 takes 7 secs to execute so it will stop when P2 arrives at 2 secs with only 4 secs to execute, so P1 has remaining 5 secs
> 
> ‚Üí P2 will also be stopped when P3 arrives at 4 secs with only 1 sec to execute
> 
> ‚Üí since P1 still has 5 secs, P4 will conduct first with 4 secs

> Turnaround Time = Completion Time ‚Äì Arrival Time Waiting Time = Turnaround Time ‚Äì Burst Time

|Process|Burst Time|Arrival Time|Turnaround Time|Waiting Time|
|---|---|---|---|---|
|P1|7|0|16 - 0 = 16|16 - 7 = 9|
|P2|4|2|7 - 2 = 5|5 - 4 = 1|
|P3|1|4|5 - 4 = 1|1 - 1 = 0|
|P4|4|5|11 - 5 = 6|6 - 4 = 2|

$ATT = \frac{16+5+1+6}{4} = 7$

$AWT = \frac{9+1+0+2}{4}=3$

- **Exercise**
    
    |Process|Burst Time|Arrival Time|Turnaround Time|Waiting Time|
    |---|---|---|---|---|
    |P1|8|0|15 - 0 = 15|15 - 8 = 7|
    |P2|3|2|5 - 2 = 3|3 - 3 = 0|
    |P3|6|4|21 - 4 = 17|17 - 6 = 11|
    |P4|4|5|9 - 5 = 4|4 - 4 = 0|
    
    ```c
    P1 ----        ----
    P2     ----
    P3                 ----
    P4         ----
      0   2   5   9   15  21
    2 4 5 9 15           
    6 6 6 6 0  
    3 1 0 0 0  
    6 6 6 6 6  
    4 4 4 0 0  
    ```
    
    $ATT = \frac{15+3+17+4}{4} = 9.75$
    
    $AWT = \frac{7+11}{4}=4.5$
    
- Exercise UI
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/3ebaa25b-c881-41dd-8c93-5ab8e4ab3e71/image.png)
    

</aside>

## Round Robin (TO PRACTICE)

<aside> üìù

- preemptive
- has time slice that tells how long a process can execute per cycle
- conducts the processes in order
- solves starvation problem

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/839874d9-3eed-40bc-8c6f-c7281ee1305e/image.png)

![the vertical process names is the queue at a certain time, the numbers is how long a process conducts before a new cycle](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/88abb74f-2c95-4952-b41c-10ca5a40c3bb/image.png)

the vertical process names is the queue at a certain time, the numbers is how long a process conducts before a new cycle

> ‚Üí P1 will be stopped at 20 secs with remaining 33 secs for execution
> 
> ‚Üí P2 will be next at arrival time 20
> 
> ‚Üí at 25 and 30 secs, P3 and P4 enters the queue in FIFO
> 
> ‚Üí P1 executes for another 20 secs then goes to the back of the line

```c
P1 ----20     ----57            ----110 
P2       ----37    
P3                  ----77            ----130       ----162
P4                        ----97             ----134
0      20      37      57      77      97      110     130    134
P1(53) P2(17)  P1(33)  P3(68)  P4(24)  P1(13)  P3(48)  P4(4)  P3(28)
       P1(33)  P3(68)  P4(24)  P1(13)  P3(48)  P4(4)   P3(28)
		           P4(24)  P1(13)  P3(48)  P4(4)
```

> Turnaround Time = Completion Time ‚Äì Arrival Time Waiting Time = Turnaround Time ‚Äì Burst Time

|Process|Burst Time|Arrival Time|Turnaround Time|Waiting Time|
|---|---|---|---|---|
|P1|53|0|110 - 0 = 110|110 - 53 = 57|
|P2|17|20|37 - 20 = 17|17 - 17 = 0|
|P3|68|25|162 - 25 = 137|137 - 68 = 69|
|P4|24|30|134 - 30 = 104|104 - 24 = 80|

$ATT = \frac{110+17+137+104}{4}=92$

$AWT = \frac{57+69+80}{4}=51.5$

- **Exercise**
    
    Round Robin with time slice=2
    
    |Process|Burst Time|Arrival Time|Turnaround Time|Waiting Time|
    |---|---|---|---|---|
    |P1|8|0|21 - 0 = 21|21 - 8 = 13|
    |P2|3|2|11 - 2 = 9|9 - 3 = 6|
    |P3|6|4|19 - 4 = 15|15 - 6 = 9|
    |P4|4|5|15 - 5 = 10|10 - 4 = 6|
    
    ```c
    P1 --2         --10            --17    --21
    P2    --4          --11             
    P3       --6           --13        --19
    P4          --8            --15
    0     2     4     6     8     10    11    13    15    17    19
    P1(8) P2(3) P3(6) P4(4) P1(6) P2(1) P3(4) P4(2) P1(4) P3(2) P1(2)
          P1(6) P1(6) P1(6) P2(1) P3(4) P4(2) P1(4) P3(2) P1(2)
                P2(1) P2(1) P3(4) P4(2) P1(4) P3(2)
                      P3(4) P4(2) P1(4)
    ```
    
    $ATT=\frac{21+9+15+10}{4}=13.75$
    
    $AWT = \frac{13+6+9+6}{4}=8.5$
    
- Exercise UI
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/ab67716b-56fb-4176-a8ca-68b0f84cf7b3/image.png)
    
- Exercise Responsi
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/a32cbdd5-668a-4400-9b48-42c42cc454f0/image.png)
    

</aside>

## The Long-Term Scheduler, The Medium-Term scheduler, The Short-Term scheduler, The I/O Scheduler (TO QUIZ)

<aside> üìù

The **Long-Term Scheduler** activates when a new job enters the system, determining when to admit it based on system load and multiprogramming levels. The **Medium-Term Scheduler** manages memory resources by deciding when to swap processes in and out of main memory, keeping the degree of multiprogramming balanced. The **Short-Term Scheduler** is the most frequently activated, as it decides which ready process in main memory should execute next, responding to events like process blocking, completion, or preemption. Lastly, the **I/O Scheduler** handles pending I/O requests, prioritizing access to available I/O devices and balancing I/O load to enhance system throughput, especially for I/O-bound processes.

- Detailed Explanation
    
    > Here‚Äôs an explanation of when each type of scheduler is activated:
    > 
    > 1. **Long-Term Scheduler**: Activated when a new job or process enters the system. It decides when to admit a new process into the pool of active processes based on factors like the system‚Äôs desired level of multiprogramming and resource availability. This scheduler manages the flow of new jobs into the system, keeping the number of concurrent processes at an optimal level for performance.
    > 2. **Medium-Term Scheduler**: Activated as part of the system‚Äôs swapping function, particularly when managing memory resources. If a process needs to be swapped out (e.g., to free up memory), this scheduler handles the process‚Äôs suspension, and when memory is available, it brings suspended processes back into main memory. This scheduling maintains control over active processes in memory and helps manage the degree of multiprogramming.
    > 3. **Short-Term Scheduler**: Activated frequently to decide which process among those ready in main memory should execute next. This scheduler is invoked whenever the currently running process is blocked, completes, or when preemption occurs (due to a clock interrupt, I/O interrupt, or OS call). Known as the dispatcher, it makes quick, fine-grained decisions to keep the processor active.
    > 4. **I/O Scheduler**: Activated when there are pending I/O requests, determining which process‚Äôs I/O request should be handled by available I/O devices. It prioritizes access to I/O resources, ensuring efficient usage and balancing system throughput, especially for I/O-bound processes.
    

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/89c55b08-d116-45cb-9165-5f8d169d32a0/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/f4945693-0c50-45cc-a183-444dbb5d2d31/798fc69a-3c98-4b33-8b6b-112d2ecc0e12.png)

![Screenshot 2024-11-03 at 9.05.04 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/78d9cc2a-b28b-495c-99fd-121c0f778e95/a34b58f0-186a-439a-a596-cbb9fe6d06b8.png)

![Screenshot 2024-11-03 at 9.05.32 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/748d5943-b0db-4112-9fa4-1f92f87a5cf0/5c32221a-793e-475e-ab3c-50bf6ee6dc93.png)

![Screenshot 2024-11-03 at 9.06.54 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/e3bd7038-70a0-4628-a7b0-5502b8813219/fe381053-1969-4a5a-bc65-4b407f3fa459.png)

</aside>

## Thread and Concurrency

<aside> üìù

- Thread is an active entity
    - executing unit of a process
- Works simultaneously with others
    - many threads executing
- Requires coordination
    - CPU, memory, I/O devices
- Threads execute the same code for diff subset of inputs
    - Different instructions at each point in time
- Can operate on different portions or functions of the code
- Has hot cache because one thread repeatedly executes one task
- Switching between threads is faster than switching between processes because threads share the same memory space
- Since threads are allowed to access and modify the same address space at the same time, this can cause inconsistencies
- Mutual Exclusion
    - execute one thread/process at a time
    - Mutex allow critical sections to be executed by one thread at a time
- Wait
    - ex. shipment after order is finalized and payment is done
- Critical section
    - A section of code that uses shared resource but can‚Äôt be run by multiple processes at the same time (writing to a file)
- Semaphore
    - non negative integer which represents the number of resources we have
    - when a process requires one the resources it reduces to 1 and once it reaches 0, they have to wait until the resource is released and the semaphore value increases
- Binary Semaphore
    - Either yes or no used for critical section
    - Mutex same to binary semaphore but mutex is different where the value is only change by the one that originally changed the value so the process that locks it should be the one that unlocks it while binary semaphore other processes can lock and unlock </aside>

## Disabling Interrupt (TO QUIZ ‚è™)

<aside> üìù

- Interrupt Disabling
- In a uniprocessor system, concurrent processes cannot have overlapped execution; they can only be interleaved
- A process will continue to run until it invokes an OS service or until it is interrupted
- Therefore, to guarantee mutual exclusion, it is sufficient to prevent a process from being interrupted
- This capability can be provided in the form of primitives defined by the OS kernel for disabling and enabling interrupts
- Disadvantages:
- The efficiency of execution could be noticeably degraded because the processor is limited in its ability to interleave processes
    - Critical section can take a while and the interrupt which possible requires a short time has to wait which is not efficient
- This approach will not work in a multiprocessor architecture
    - Interrupt only 1 processor
    - Cannot control interrupts to multiple processors

> In a **uniprocessor system**, ensuring mutual exclusion in critical sections can be achieved by **disabling interrupts**, preventing other processes from interrupting execution within those sections. However, this approach, while effective in preventing race conditions, **limits process interleaving**, reducing efficiency, and isn‚Äôt feasible in **multiprocessor systems** where multiple processes may run concurrently across CPUs. **Interrupts** are external signals, typically from hardware devices, that notify the CPU of events like network packet arrivals or timeouts, while **signals** are software or CPU-triggered events indicating issues like memory access violations. Both can be selectively disabled with **masks** to avoid deadlocks, particularly when an interrupt or signal handler might require a **mutex** that‚Äôs already held by the interrupted thread. **Masks**, sequences of bits that enable or disable specific interrupts or signals, ensure critical sections remain uninterrupted by keeping interrupt handlers pending until the critical section is complete and the mask is reset. In multi-CPU systems, interrupt masks are managed per CPU, blocking specific interrupts from reaching a CPU based on the mask‚Äôs configuration, thus preventing simultaneous interruptions. However, **interrupt disabling** comes with trade-offs: it may introduce **latency** and **missed interrupts**, increasing system complexity, especially in multi-CPU setups, as each CPU‚Äôs mask needs careful management to maintain timely event processing and avoid underutilizing resources.

> In a **uniprocessor system**, concurrent processes cannot execute simultaneously but can interleave. To ensure **mutual exclusion** in critical sections, the OS can disable interrupts, preventing any process interruptions until the section is complete. This approach guarantees mutual exclusion but reduces efficiency and doesn‚Äôt scale to **multiprocessor architectures**. In multiprocessor systems, **interrupt disabling** alone cannot ensure mutual exclusion since multiple processors may execute processes concurrently.
> 
> **Interrupts** and **signals** provide essential notifications to the CPU. **Interrupts** are external notifications, such as from I/O devices, that alert the CPU to handle events independently of the CPU‚Äôs current tasks. **Signals** are triggered by software or CPU activities, used to handle internal issues or actions like memory violations. While both have **unique identifiers** and can be selectively disabled using **masks**, they differ in scope. Interrupts affect the whole CPU and are controlled at the **CPU level**, whereas signals are specific to individual processes. Masks prevent deadlocks, especially if an interrupt or signal handler requires a **mutex** already held by the interrupted thread.
> 
> By disabling interrupts or signals temporarily, masks ensure that **critical sections** remain uninterrupted, thus preventing deadlocks. Interrupt masks are specific to each CPU and, if set, block the hardware interrupt routing mechanism from delivering interrupts to that CPU. This mechanism stabilizes critical code but has drawbacks, such as **increased latency** and the risk of **missed interrupts**. **Multi-CPU systems** face additional complexity, as each CPU‚Äôs mask must be managed carefully to avoid missed tasks and system delays.

> Here‚Äôs a simplified summary explaining **interrupts** and **signals** and their key differences:
> 
> ### Interrupts
> 
> - **What They Are**: External notifications sent to the CPU, often from hardware devices (e.g., network cards or timers).
> - **Purpose**: They alert the CPU to handle an external event, like a network packet arrival or a timeout.
> - **Platform Dependence**: The types of interrupts vary by hardware setup and device configuration.
> - **Asynchronous**: Interrupts can happen at any time, without any direct action on the CPU.
> 
> ### Signals
> 
> - **What They Are**: Events triggered by software or by the CPU itself.
> - **Purpose**: They often indicate specific issues or actions in the software, like a memory access violation or program exceptions.
> - **OS Dependent**: Signals differ depending on the operating system, even on identical hardware.
> - **Synchronous and Asynchronous**:
>     - **Synchronous**: Triggered by specific actions, like a process accessing unauthorized memory.
>     - **Asynchronous**: Can also be triggered independently of specific CPU actions.
> 
> ### Similarities
> 
> - **Unique Identifiers**: Both have identifiers, determined by hardware for interrupts and by the OS for signals.
> - **Masks**:
>     - **Interrupt Mask (per CPU)**: Controls interrupt delivery to the CPU.
>     - **Signal Mask (per process)**: Controls signal delivery to specific processes.
> - **Handlers**:
>     - **Interrupt Handlers**: Set by the OS for the entire system.
>     - **Signal Handlers**: Set by individual processes, with each process specifying its own response to different signals.
> 
> ### Summary
> 
> - **Interrupts** notify the CPU about external events and are handled system-wide, controlled by the OS and bound to the CPU.
> - **Signals** are software or CPU-triggered events, handled within each process, allowing more specific responses to particular conditions.

> Here‚Äôs a simplified explanation of why interrupts and signals are sometimes disabled, using **masks** to prevent issues like **deadlocks**:
> 
> ### Why Disable Interrupts or Signals?
> 
> Interrupts and signals are handled within the context of the currently running thread. This means they use the **thread's stack** and interrupt the current execution. Problems arise if the handling routine (the code triggered by the interrupt or signal) tries to use a **mutex** (a lock that prevents data conflicts), especially if the thread that was interrupted is already holding that mutex.
> 
> ### Example of a Deadlock:
> 
> 1. **Interrupt or Signal Happens**: The thread‚Äôs execution is interrupted, and control jumps to the handler code.
> 2. **Handler Needs a Mutex**: The handler code requires the same mutex that the interrupted thread is holding.
> 3. **Deadlock**: Since the handler can‚Äôt complete until it gets the mutex (which is locked by the interrupted thread), and the interrupted thread can‚Äôt resume until the handler finishes, they‚Äôre stuck in a deadlock.
> 
> ### Solution: Using Masks
> 
> To prevent deadlocks, we can **disable** interrupts or signals temporarily. This is done using **masks**:
> 
> - **Masks**: A bit sequence where each bit controls whether a specific interrupt or signal is enabled or disabled.
> - **How it Works**:
>     - **Before Acquiring a Mutex**: The thread disables certain interrupts or signals (sets specific bits in the mask).
>     - **During Critical Section**: If an interrupt or signal occurs, it remains pending until the mask is reset, ensuring it doesn‚Äôt interrupt the critical section.
>     - **After Releasing the Mutex**: The mask is reset, enabling the previously disabled interrupts or signals. If any are pending, the handler is now allowed to execute.
> 
> This approach ensures:
> 
> - **Critical Sections Remain Uninterrupted**: Important code sections (where the mutex is held) are safe from interruptions that could cause deadlocks.
> - **Pending Events are Handled Safely**: Once the critical section is done, any pending interrupts or signals can be processed without risk of conflict.
> 
> ### Summary:
> 
> By disabling interrupts and signals with masks, we avoid situations where a handler tries to acquire a mutex already held by the interrupted thread, thus preventing deadlocks. Once the critical work is complete and the mutex is released, the mask allows pending interrupts or signals to be processed safely.

> Here‚Äôs an expanded explanation of interrupt masks, why they‚Äôre useful, and some disadvantages to consider:
> 
> ### How Interrupt Masks Work
> 
> Interrupt masks are managed **per CPU**, meaning each CPU has its own mask. When an interrupt is disabled via a mask:
> 
> - **Interrupt Delivery is Blocked**: The CPU won‚Äôt receive that specific interrupt because the hardware routing mechanism simply doesn‚Äôt send it to the CPU.
> 
> ### Why Use Interrupt Masks
> 
> Using interrupt masks is beneficial because:
> 
> - **Prevents Deadlocks**: As discussed, disabling interrupts avoids issues where handler code needs a mutex already held by the interrupted thread.
> - **Improves Stability**: Critical sections (where data integrity is key) remain uninterruptible, making the system more reliable.
> - **Custom Control**: Each CPU can control which interrupts it receives, providing flexibility in managing workloads and responsiveness.
> 
> ### Disadvantages of Interrupt Masks
> 
> Despite these benefits, there are some drawbacks:
> 
> 1. **Potentially Missed Interrupts**:
>     - If an interrupt is masked, the CPU won‚Äôt receive it, which may delay handling important events.
>     - If too many interrupts are masked for too long, the system might fall behind on time-sensitive tasks.
> 2. **Increased Latency**:
>     - Since certain interrupts are deferred until masks are reset, handling them later can introduce delays in responding to critical tasks.
>     - For systems needing real-time performance, such as in medical or industrial applications, increased latency can impact reliability.
> 3. **Complexity in Multi-CPU Systems**:
>     - Each CPU having its own mask requires careful management in multi-CPU setups to avoid missed or delayed interrupts across different CPUs.
>     - Coordinating masks between CPUs can be complex, particularly in highly parallel systems.
> 4. **Resource Wastage**:
>     - If interrupts are disabled too frequently, CPUs may idle waiting for tasks that are masked out, underutilizing hardware resources.
> 
> ### Summary
> 
> Interrupt masks provide fine control over interrupt handling on a per-CPU basis, helping to prevent deadlocks and keep critical sections stable. However, they can lead to missed interrupts, increased latency, and added complexity in multi-CPU systems, which requires careful balancing to avoid performance issues.

</aside>

## Semaphores (TO QUIZ ‚è™)

<aside> üìù

**Semaphore**

- Mutual Exclusion
    - execute one thread/process at a time
    - Mutex allow critical sections to be executed by one thread at a time
- Wait
    - ex. shipment after order is finalized and payment is done
- Critical section
    - A section of code that uses shared resource but can‚Äôt be run by multiple processes at the same time (writing to a file)
- Semaphore
    - non negative integer which represents the number of resources we have
    - when a process requires one the resources it reduces to 1 and once it reaches 0, they have to wait until the resource is released and the semaphore value increases
- Binary Semaphore
    - Either yes or no used for critical section
    - Mutex same to binary semaphore but mutex is different where the value is only change by the one that originally changed the value so the process that locks it should be the one that unlocks it while binary semaphore other processes can lock and unlock

> A **semaphore** is a synchronization tool used to control access to shared resources in concurrent processing. It has an integer value that can be modified only through three atomic operations: initialization, `semWait` (decrement), and `semSignal` (increment). This counter-based approach allows semaphores to either count available resources or enforce mutual exclusion, ensuring one process or thread executes in a critical section at a time. There are two main types of semaphores:
> 
> 1. **Counting Semaphore (General Semaphore)**: This type has a nonnegative integer value, tracking the number of available resources. When a process executes `semWait`, it decrements the value. If the value is zero, the process must wait. Once another process completes its critical section, it calls `semSignal`, incrementing the semaphore and allowing another waiting process to proceed.
> 2. **Binary Semaphore (Mutex)**: This operates with values of 0 or 1, functioning similarly to a mutex. A process that wants access to a critical section performs `semWait`. If the semaphore value is 1, it decrements to 0, allowing entry to the critical section. If the value is already 0, other processes must wait. When the process exits, it calls `semSignal` to set the semaphore back to 1, enabling the next waiting process to enter.
> 
> ### Mutual Exclusion with Semaphores
> 
> To enforce mutual exclusion, a binary semaphore is typically initialized to 1. Before entering a critical section, a process calls `semWait`, decrementing the semaphore. If it is already 0, the process waits until it becomes available. When the critical section is complete, the process calls `semSignal`, allowing other processes access.
> 
> ### Solving the Producer-Consumer Problem
> 
> The Producer-Consumer Problem demonstrates semaphore use in managing resource access. Here, producers add items to a limited buffer, while consumers remove them. The solution involves three semaphores:
> 
> - `mutex` (binary semaphore) for mutual exclusion,
> - `empty` (counting semaphore) tracking buffer space, and
> - `full` (counting semaphore) tracking filled slots.
> 
> ```c
> semaphore mutex = 1;
> semaphore empty = N; // N = buffer capacity
> semaphore full = 0;
> 
> void producer() {
>     while (true) {
>         wait(empty);       // Wait for an empty slot
>         wait(mutex);       // Lock buffer
>         // Add item to buffer
>         signal(mutex);     // Unlock buffer
>         signal(full);      // Signal that a slot is full
>     }
> }
> 
> void consumer() {
>     while (true) {
>         wait(full);        // Wait for a filled slot
>         wait(mutex);       // Lock buffer
>         // Remove item from buffer
>         signal(mutex);     // Unlock buffer
>         signal(empty);     // Signal an empty slot
>     }
> }
> 
> ```
> 
> Here:
> 
> - **`empty`** prevents producers from overfilling the buffer.
> - **`full`** prevents consumers from accessing an empty buffer.
> - **`mutex`** ensures mutual exclusion, allowing only one process (producer or consumer) to access the buffer at a time.
> 
> Through this setup, semaphores facilitate efficient resource management, avoiding data corruption and ensuring that producers and consumers operate smoothly without interfering.

> Here‚Äôs an expanded explanation of **semaphores** with examples, focusing on their role in **mutual exclusion** and solving the **Producer-Consumer Problem**:
> 
> ### Understanding Semaphores
> 
> A **semaphore** is a synchronization tool that controls access to a shared resource using a counter. It helps manage how many threads can access a resource at a given time, and it has two primary operations:
> 
> - **Wait (P)**: Decreases the semaphore value if it‚Äôs greater than zero, allowing the thread to proceed.
> - **Signal (V)**: Increases the semaphore value, allowing another thread to proceed.
> 
> ### Mutual Exclusion with Semaphores
> 
> Semaphores can enforce **mutual exclusion**, where only one thread at a time accesses a shared resource. Here‚Äôs how:
> 
> - **Binary Semaphore for Mutual Exclusion**:
>     - A binary semaphore (initialized to 1) behaves like a mutex.
>     - When a thread enters a critical section, it performs the **Wait** operation, decrementing the semaphore to 0.
>     - If another thread tries to enter, it will see the semaphore at 0 and wait until the first thread performs the **Signal** operation to increment it back to 1.
> 
> This way, only one thread can access the critical section at a time, ensuring **mutual exclusion**.
> 
> ### Example: Solving the Producer-Consumer Problem
> 
> The **Producer-Consumer Problem** is a classic synchronization problem. Here, we have:
> 
> - **Producers** that add items to a buffer (shared resource).
> - **Consumers** that remove items from the buffer.
> 
> The buffer has a limited capacity, so synchronization is necessary to:
> 
> 1. Ensure producers don‚Äôt add items when the buffer is full.
> 2. Ensure consumers don‚Äôt remove items when the buffer is empty.
> 
> We can solve this with **three semaphores**:
> 
> 1. **`mutex`**: A binary semaphore to ensure mutual exclusion while accessing the buffer.
> 2. **`empty`**: A counting semaphore initialized to the buffer‚Äôs capacity, indicating how many empty slots are in the buffer.
> 3. **`full`**: A counting semaphore initialized to 0, indicating how many items are in the buffer.
> 
> ### Code Example for the Producer-Consumer Problem
> 
> ```c
> semaphore mutex = 1; // Ensures mutual exclusion
> semaphore empty = N; // Tracks empty slots in buffer, initialized to buffer capacity
> semaphore full = 0;  // Tracks filled slots in buffer, initialized to 0
> 
> // Producer
> void producer() {
>     while (true) {
>         // Produce an item
>         wait(empty);         // Decrement empty count
>         wait(mutex);         // Lock buffer access
>         // Add item to buffer
>         signal(mutex);       // Unlock buffer
>         signal(full);        // Increment count of full slots
>     }
> }
> 
> // Consumer
> void consumer() {
>     while (true) {
>         wait(full);          // Decrement count of full slots
>         wait(mutex);         // Lock buffer access
>         // Remove item from buffer
>         signal(mutex);       // Unlock buffer
>         signal(empty);       // Increment count of empty slots
>         // Consume the item
>     }
> }
> 
> ```
> 
> ### Explanation of the Solution
> 
> 1. **Mutual Exclusion**:
>     - The `mutex` semaphore ensures only one thread (either a producer or a consumer) accesses the buffer at any time, preventing data corruption.
> 2. **Buffer Management**:
>     - **`empty`**: Keeps track of empty slots. When a producer adds an item, it decrements `empty`. If `empty` is 0, the producer waits until there‚Äôs space.
>     - **`full`**: Tracks the number of items in the buffer. When a consumer removes an item, it decrements `full`. If `full` is 0, the consumer waits until there‚Äôs something to consume.
> 
> ### How Semaphores Solve the Problem
> 
> - **Producers wait on `empty`**: Prevents them from overfilling the buffer.
> - **Consumers wait on `full`**: Prevents them from removing items from an empty buffer.
> - **Mutex for Mutual Exclusion**: Ensures only one producer or consumer accesses the buffer at any moment, avoiding conflicts.
> 
> ### Summary
> 
> Semaphores are powerful tools for managing concurrent access to resources. By using counting and binary semaphores, we ensure:
> 
> - **Mutual exclusion** (using `mutex`).
> - **Correct buffer management** (using `empty` and `full`).
> 
> This method allows producers and consumers to work efficiently without interfering with each other, solving the Producer-Consumer Problem effectively.

</aside>

## Produce-Consumer Problem (TO QUIZ ‚è™)

<aside> üìù

ex: initialize 3 semaphore 1 for mutex, empty for the size of bufffer, full for 0

> The **Producer/Consumer Problem** is a classic example in concurrent processing, where one or more producers generate data and place it into a shared buffer, while a single consumer removes data from that buffer. Only one producer or consumer can access the buffer at a time to prevent data inconsistency, ensuring **mutual exclusion**. The primary challenge here is to prevent the producer from adding data when the buffer is full and to stop the consumer from removing data when the buffer is empty. This problem can be solved effectively using **semaphores** to manage buffer access.
> 
> In this setup, three semaphores are used:
> 
> 1. **Empty** (initialized to the buffer size) keeps track of available slots.
> 2. **Full** (initialized to 0) counts the filled slots in the buffer.
> 3. **Mutex** (initialized to 1) ensures that only one process (producer or consumer) accesses the critical section (buffer) at any time.
> 
> The producer, before adding an item, first checks the **Empty** semaphore to ensure there‚Äôs space, then acquires the **Mutex** to enter the critical section, adds the item, releases the **Mutex**, and signals **Full** to notify the consumer. Similarly, the consumer checks **Full** to ensure there‚Äôs an item to consume, acquires the **Mutex** to safely remove the item, and then signals **Empty** to indicate a free slot in the buffer. This approach using semaphores guarantees mutual exclusion, avoids buffer overflows or underflows, and enables the producer and consumer to operate concurrently.

> The **Producer-Consumer problem** addresses the need for mutual exclusion in concurrent processing, where one or more producers generate data and place it into a buffer, while a single consumer retrieves the data from the buffer. Mutual exclusion is essential here, as only one producer or consumer can access the buffer at a time to avoid conflicts. This problem requires synchronization to ensure the producer doesn't add data if the buffer is full, and the consumer doesn't remove data from an empty buffer.
> 
> To solve this, **semaphores** are used. Three semaphores are typically initialized: one for tracking empty spaces, one for the number of full slots, and another to control mutual exclusion within the buffer. For instance, if the buffer has a size `N`, the semaphore `empty` is set to `N`, representing empty slots, and `full` is set to 0, representing the absence of filled slots initially. The semaphore `mutex` is initialized to 1 to allow single access at a time.
> 
> The producer checks the `empty` semaphore to see if there's space available. If so, it decrements `empty`, enters the critical section controlled by `mutex`, appends data to the buffer, then increments `full` to signal an available item. The consumer operates similarly but checks `full` before trying to consume an item, decrementing it to ensure there‚Äôs an item available. Once it has taken an item, it increments `empty`, signaling a space is free.
> 
> This synchronization model allows **concurrent execution** while managing buffer availability and ensuring safe access through mutual exclusion. However, careful implementation is required to avoid issues like deadlocks or resource contention, especially if semaphores are used incorrectly or out of order

> The Producer-Consumer Problem involves managing a shared buffer where one or more producers generate items and place them in the buffer, while a single consumer removes items. This setup requires a mechanism to ensure that only one producer or consumer accesses the buffer at any time (mutual exclusion) and that the producer does not add items to a full buffer, nor does the consumer remove items from an empty buffer.
> 
> A common solution to this problem uses semaphores. Three semaphores are initialized:
> 
> 1. **Empty** (for tracking empty slots in the buffer),
> 2. **Full** (for items present in the buffer), and
> 3. **Mutex** (to control access to the critical section of the buffer).
> 
> ### Solution Steps
> 
> 1. **Producer Process**:
>     - The producer checks the `empty` semaphore to ensure there is space in the buffer.
>     - It then acquires the `mutex` semaphore, enters the critical section, and adds an item to the buffer.
>     - After adding the item, it releases `mutex` and signals `full`, indicating the buffer has one more item.
> 2. **Consumer Process**:
>     - The consumer first checks `full` to confirm there are items available.
>     - It acquires `mutex` to enter the critical section and remove an item from the buffer.
>     - After consuming the item, it releases `mutex` and signals `empty` to show a slot is now free in the buffer.
> 
> ### Example Pseudocode
> 
> - **Producer**:
>     
>     ```c
>     while (true) {
>         produce();
>         semWait(empty); // Wait for an empty slot
>         semWait(mutex); // Lock the critical section
>         append();       // Add item to buffer
>         semSignal(mutex); // Unlock the critical section
>         semSignal(full); // Signal the presence of a new item
>     }
>     
>     ```
>     
> - **Consumer**:
>     
>     ```c
>     while (true) {
>         semWait(full); // Wait for an available item
>         semWait(mutex); // Lock the critical section
>         take();         // Remove item from buffer
>         semSignal(mutex); // Unlock the critical section
>         semSignal(empty); // Signal an empty slot
>         consume();
>     }
>     
>     ```
>     
> 
> In this way, the `empty`, `full`, and `mutex` semaphores coordinate the producer and consumer's actions, ensuring efficient and deadlock-free access to the buffer. The semaphore values are carefully managed to prevent buffer overflow (producers adding to a full buffer) and underflow (consumers taking from an empty buffer), making this a robust approach to the Producer-Consumer Problem.

> To solve the Producer-Consumer Problem using semaphores, three key semaphores are employed to manage access to a shared buffer between producer(s) and a single consumer. The problem requires ensuring that producers don‚Äôt add items to a full buffer and consumers don‚Äôt remove items from an empty buffer, while also preventing simultaneous access to the buffer (mutual exclusion).
> 
> ### Semaphores Used:
> 
> 1. **Empty**: Tracks the number of empty slots in the buffer. It starts at the buffer‚Äôs capacity, allowing the producer to add items only when there is available space.
> 2. **Full**: Tracks the number of filled slots in the buffer. It starts at zero, indicating that the buffer is empty initially. The consumer can only remove items when this value is greater than zero.
> 3. **Mutex**: Controls access to the critical section, where items are added to or removed from the buffer, ensuring only one producer or consumer accesses the buffer at any time.
> 
> ### How It Works:
> 
> - **Producer Process**:
>     - The producer first waits on `Empty` to ensure there is space in the buffer. If `Empty` is zero, the producer blocks, waiting for the consumer to consume an item.
>     - After confirming space, the producer waits on `Mutex` to enter the critical section, adding an item to the buffer.
>     - After adding the item, the producer signals `Mutex` to release the critical section and then signals `Full`, indicating an additional item is available for the consumer.
> - **Consumer Process**:
>     - The consumer waits on `Full` to ensure there is an item to consume. If `Full` is zero, the consumer blocks, waiting until the producer adds an item.
>     - After confirming an item is available, the consumer waits on `Mutex` to enter the critical section, removing an item from the buffer.
>     - After consuming the item, the consumer signals `Mutex` to release the critical section and then signals `Empty`, indicating an additional space is available for the producer.
> 
> ### Example Pseudocode
> 
> - **Producer**:
>     
>     ```c
>     while (true) {
>         produce();
>         semWait(empty); // Wait for an empty slot
>         semWait(mutex); // Lock the critical section
>         append();       // Add item to buffer
>         semSignal(mutex); // Unlock the critical section
>         semSignal(full); // Signal the presence of a new item
>     }
>     
>     ```
>     
> - **Consumer**:
>     
>     ```c
>     while (true) {
>         semWait(full); // Wait for an available item
>         semWait(mutex); // Lock the critical section
>         take();         // Remove item from buffer
>         semSignal(mutex); // Unlock the critical section
>         semSignal(empty); // Signal an empty slot
>         consume();
>     }
>     
>     ```
>     
> 
> ### Summary:
> 
> Using these three semaphores ensures mutual exclusion within the buffer, prevents buffer overflow and underflow, and allows for smooth, deadlock-free operation of concurrent producer and consumer processes. This approach effectively synchronizes producer and consumer actions, ensuring each process proceeds only when conditions are safe and resources are available.

</aside>

## Deadlock (TO PRACTICE ‚è™)

<aside> üìù

### Sample Question

Two processes, **R** and **S**, require exclusive access to two resources, **X** and **Y**. The program's structure for each process is shown below. Determine why deadlock occurs and modify the code to prevent it.

**Process R**

|Time|Action|
|---|---|
|1|Request X|
|2|Lock X|
|3|Request Y|
|4|Lock Y|
|5|Unlock X|
|6|Unlock Y|

**Process S**

|Time|Action|
|---|---|
|1|Request Y|
|2|Lock Y|
|3|Request X|
|4|Lock X|
|5|Unlock Y|
|6|Unlock X|

### Solution

### Step 1: Identifying the Deadlock Problem

In this example, **Process R** locks **X** at time 2 and then requests **Y** at time 3. Meanwhile, **Process S** locks **Y** at time 2 and then requests **X** at time 3. Because each process is holding one resource and waiting for the other, they enter a deadlock situation. This deadlock arises due to the following conditions:

1. **Mutual Exclusion**: Each resource (either **X** or **Y**) can only be held by one process at a time.
2. **Hold and Wait**: Each process holds one resource while waiting for the other.
3. **No Preemption**: A resource can only be released voluntarily by the process holding it.
4. **Circular Wait**: **R** holds **X** and waits for **Y**, while **S** holds **Y** and waits for **X**.

### Step 2: Modifying the Code to Prevent Deadlock

To avoid deadlock, we can change the order in which resources are requested. By making both **Process R** and **Process S** acquire **X** before **Y**, we eliminate the possibility of a circular wait. This ensures that if one process has **X**, the other process will wait for **X** instead of acquiring **Y**.

Here's how the modified code for both processes would look:

**Modified Process R**

|Time|Action|
|---|---|
|1|Request X|
|2|Lock X|
|3|Request Y|
|4|Lock Y|
|5|(Work with X and Y)|
|6|Unlock Y|
|7|Unlock X|

**Modified Process S**

|Time|Action|
|---|---|
|1|Request X|
|2|Lock X|
|3|Request Y|
|4|Lock Y|
|5|(Work with X and Y)|
|6|Unlock Y|
|7|Unlock X|

### Explanation

By enforcing the order in which **R** and **S** request resources, deadlock is avoided because both processes follow a consistent locking order. If **Process R** has locked **X**, **Process S** will have to wait for **X** before it can request **Y**, thus avoiding the circular wait condition that caused the deadlock originally.

</aside>

## Banker‚Äôs Algorithm (TO PRACTICE )

<aside> üìù

![allocate available resource to P2 to finish and P2 resources will be available](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/b15e0b33-83af-47f4-ba93-81b6e5d84c12/Screenshot_2024-11-04_at_11.28.09_AM.png)

allocate available resource to P2 to finish and P2 resources will be available

![now its in a safe state since all processes will work because available vector is greater than need matrix](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/fa88af6d-0b53-4dd9-88ac-5c49cdefa2bc/Screenshot_2024-11-04_at_11.28.25_AM.png)

now its in a safe state since all processes will work because available vector is greater than need matrix

<aside> ‚ùì

What if vector R is not given? Assume

</aside>

Given Claim/Max Matrix $C$, Allocation Matrix $A$, and Resource Vector $R$

Find Available vector $V$ by $R-\sum{A}$ per column

Create Need Matrix $C-A$ `(to compare with available)`

If process requires less than available

complete the process and empty everything

add process row in Allocation $A$ to Available vector $V$

- Example
    
    ![Screenshot 2024-11-04 at 2.40.39 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/97fab2ac-a7b6-4a6d-902c-4c4cd839b70a/Screenshot_2024-11-04_at_2.40.39_PM.png)
    
- Exercise
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/5d2a3edd-e499-4257-9fae-a7e7046811f3/image.png)
    

</aside>

## Resource Allocation Graph

<aside> üìù

basically it is used to map processes to resources and the direction of the arrows indicate whether it is requesting for a resource or it is handling a resource

we can detect a deadlock using these rules:

if there is no cycle, then there is no deadlock

if there is a cycle + one resource type and all the processes request the resource that each process handle it will result in a deadlock

however, if there is a cycle + more than one instance of resource type, there may be a deadlock

![https://www.youtube.com/watch?v=qPuf5B5xPCs&ab_channel=Jenny'sLecturesCSIT](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/a3178976-b25a-47e8-996f-a9a9c305461c/image.png)

[](https://www.youtube.com/watch?v=qPuf5B5xPCs&ab_channel=Jenny%27sLecturesCSIT)[https://www.youtube.com/watch?v=qPuf5B5xPCs&ab_channel=Jenny'sLecturesCSIT](https://www.youtube.com/watch?v=qPuf5B5xPCs&ab_channel=Jenny'sLecturesCSIT)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/49d5f04e-3447-4822-87b3-56c19b1813b8/ec06531a-8d18-472f-ae9e-b73e7586aca9/image.png)

</aside>